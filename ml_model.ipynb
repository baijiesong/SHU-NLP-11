{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix\n",
    "\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./spam.csv', encoding='ISO-8859-1')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',None)\n",
    "\n",
    "data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)\n",
    "data.rename(columns={'v1':'label','v2':'text'},inplace=True)\n",
    "\n",
    "print('The dataset has {} rows and {} columns.'.format(data.shape[0],data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.duplicated(keep='first')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=data,x='label')\n",
    "\n",
    "plt.title('Distribution of Target Classes',fontsize=25)\n",
    "plt.xlabel('target classes',fontsize=15)\n",
    "plt.ylabel('count',fontsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(data[data['label']=='ham']['text'])\n",
    "wordcloud = WordCloud(max_font_size=50,max_words=40).generate(text)\n",
    "\n",
    "# Generate plot\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Wordcloud for 'Ham' Messages\",fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(data[data['label']=='spam']['text'])\n",
    "wordcloud = WordCloud(max_font_size=50,max_words=40).generate(text)\n",
    "\n",
    "# Generate plot\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Wordcloud for 'Spam' Messages\",fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_scores(model_name:str,preds,y_test_data):\n",
    "    '''\n",
    "    Generate a table of test scores.\n",
    "\n",
    "    In:\n",
    "        model_name (string): Your choice: how the model will be named in the output table\n",
    "        preds: numpy array of test predictions\n",
    "        y_test_data: numpy array of y_test data\n",
    "\n",
    "    Out:\n",
    "        table: a pandas df of precision, recall, f1, and accuracy scores for your model\n",
    "    '''\n",
    "    accuracy  = accuracy_score(y_test_data,preds)\n",
    "    precision = precision_score(y_test_data,preds,average='macro')\n",
    "    recall    = recall_score(y_test_data,preds,average='macro')\n",
    "    f1        = f1_score(y_test_data,preds,average='macro')\n",
    "\n",
    "    table = pd.DataFrame({'model': [model_name],'precision': [precision],'recall': [recall],\n",
    "                          'F1': [f1],'accuracy': [accuracy]})\n",
    "\n",
    "    return table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The option \"decode_error='ignore'\" is set to take care of the wrongly decoded characters\n",
    "featurizer = CountVectorizer(decode_error='ignore')\n",
    "\n",
    "X_train2 = featurizer.fit_transform(X_train)\n",
    "X_test2 = featurizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train2,y_train)\n",
    "\n",
    "MNB_train_preds = MNB.predict(X_train2)\n",
    "MNB_train_results = get_test_scores('MNB (train)',MNB_train_preds,y_train)\n",
    "\n",
    "MNB_test_preds = MNB.predict(X_test2)\n",
    "MNB_test_results = get_test_scores('MNB (test)',MNB_test_preds,y_test)\n",
    "\n",
    "MNB_results = pd.concat([MNB_train_results,MNB_test_results],axis=0)\n",
    "MNB_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate array of values for confusion matrix\n",
    "cm = confusion_matrix(y_test,MNB_test_preds,labels=MNB.classes_)\n",
    "\n",
    "ax = sns.heatmap(cm,annot=True)\n",
    "ax.set_title('Confusion Matrix (CountVectorizer + MultinomialNB)',fontsize=16)\n",
    "ax.xaxis.set_ticklabels(['ham','spam'],fontsize=12) \n",
    "ax.yaxis.set_ticklabels(['ham','spam'],fontsize=12) \n",
    "ax.set_xlabel(\"Predicted\",fontsize=14)\n",
    "ax.set_ylabel(\"Target\",fontsize=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_train = MNB.predict_proba(X_train2)[:,1]\n",
    "prob_test  = MNB.predict_proba(X_test2)[:,1]\n",
    "\n",
    "print(\"train AUC:\",roc_auc_score(y_train,prob_train))\n",
    "print(\"test AUC:\",roc_auc_score(y_test,prob_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y_train2 = encoder.fit_transform(y_train) \n",
    "y_test2  = encoder.fit_transform(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc_curve(true_y,y_prob,text):\n",
    "    \"\"\"\n",
    "    plots the roc curve based of the probabilities\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(true_y,y_prob)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.title(f'ROC Curve {text}')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_roc_curve(y_test2,prob_test,'(CountVectorizer + MultinomialNB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = TfidfVectorizer(decode_error='ignore')\n",
    "\n",
    "X_train3 = featurizer.fit_transform(X_train)\n",
    "X_test3  = featurizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train3,y_train)\n",
    "\n",
    "MNB_train_preds2   = MNB.predict(X_train3)\n",
    "MNB_train_results2 = get_test_scores('MNB (train)',MNB_train_preds2,y_train)\n",
    "\n",
    "MNB_test_preds2   = MNB.predict(X_test3)\n",
    "MNB_test_results2 = get_test_scores('MNB (test)',MNB_test_preds2,y_test)\n",
    "\n",
    "MNB_results2 = pd.concat([MNB_train_results2,MNB_test_results2],axis=0)\n",
    "MNB_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate array of values for confusion matrix\n",
    "cm = confusion_matrix(y_test,MNB_test_preds2,labels=MNB.classes_)\n",
    "\n",
    "ax = sns.heatmap(cm,annot=True)\n",
    "ax.set_title('Confusion Matrix (CountVectorizer + MultinomialNB)',fontsize=16)\n",
    "ax.xaxis.set_ticklabels(['ham','spam'],fontsize=12) \n",
    "ax.yaxis.set_ticklabels(['ham','spam'],fontsize=12) \n",
    "ax.set_xlabel(\"Predicted\",fontsize=14)\n",
    "ax.set_ylabel(\"Target\",fontsize=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
